# Certfication Challenge

## Defining Your Problem & Audience

### Write a succinct 1-sentence description of the problem

Site selection and due diligence for utility-scale solar projects is wrought with uncertainty and risk, resulting in extremely low project success rates.

### Write 1-2 paragraphs on why this is a problem for your specific user

Solar developers are faced with many layers of risk. In order to successfully get a project off the ground, they must find and secure control of a site, ensure the viability of grid interconnection, work with local authorities to attain necessary permits, satisfy environmental regulations and validate site design. Each of these stages come with their own set of unique risks, and may require significant capital expenditure. Most importantly - a single issue across any of these stages could bury a project or make it financially infeasible. 

Selecting high-potential site candidates and managing the entirety of the development process across several projects is an organizational nightmare. It is extremely time consuming and expensive. 


## Propose A Solution

### Write 1-2 paragraphs on your proposed solution. How will it look and feel to the user?

Managing risks associated with large scale solar development will continue to require manual, time consuming and/or expensive processes. However, designing an intelligent site selection workflow can have outsized effects on project success rates, risk reduction and financial outcomes.

My proposed solution would provide solar developers with a suite of tools to manage early-stage site selection risk. Some of these tools are beyond the scope for this certification challenge; however, I plan on implementing them for the final project. A major component of the system would be a chat-interface where a user can ask questions related to large-scale solar development in the state of Massachusetts. This could be questions about a particular municipalities regulations on solar and zoning, county-level regulations or statewide laws and initiatives. A user will also be able to query for solar sites meeting defined project requirements and receive a set of candidate sites meeting these crtieria (example below). Additionally, once a set of candidate sites has been found, a user can retrieve parcel specific information (i.e. solar irradiance, parcel owner, distance to nearest substation, etc..). A site-specific report may also be generated by the user, detailing many of the important details and risk factors for a particular site. 


**Example**

From Paces Website: Us: "a 2-megawatt ground-mount or freestanding solar system that requires about 12 acres of land and sells power back to the electricity grid such that a majority of electricity produced is not consumed on site and it is used for commercial purposes”

### Describe the tools you plan to use in each part of your stack. Write one sentence on why you made each tooling choice.

#### LLM
Extract information from regulatory documents
Parse user queries to route them appropriately
Extract address text from user queries for site-specific questions (i.e. site solar irradiance)


#### Embedding Model
OpenAI's text-embedding-3-small embeddings, as they are powerful and cost-effective

#### Orchestration
For the purposes of this certification challenge, I implemented a very simple orchestration component. User queries were routed in one of two directions: 

1. Site specific queries to obtain solar-irradiance data 

2. Queries which request information from municipality-level regulatory documents

#### Vector Database
Contain all the municipality, county and state-level regulatory documents relevant to solar development


#### Monitoring

I am using Langsmith to monitor the application

#### Evaluation
Used RAGAS testset generator to generate test cases and evaluated performance on the following metrics: context precision, context recall, faithfullness and response relevance


#### User Interface

The initial user interface will be a simple chat interface. Additional UI features will be added, including a map displaying candidate parcels

(Optional) Serving & Inference


## Dealing With The Data

### Describe all of your data sources and external APIs, and describe what you’ll use them for.

#### Initial Data Sources

- Massachusetts Municipality-Level Regulations


#### Initial External APIs

- NREL Solar API
- Nominatim Geocoder

### Describe the default chunking strategy that you will use. Why did you make this decision?

I decided to implement a ```RecursiveCharacterTextSplitter``` chunker with ```chunk_size = 1000``` and ```chunk_overlap = 200```. Legal documents are quite structured, and while  relevant information does exist across many different parts of a document, typically different sections contain material that is fairly sementically separate from other sections. I set a value of ```1000``` for the ```chunk_size``` based on my observation of chcracter lengths for larger than average sections of text. And I set the ```chunk_overlap``` to ```200``` without too much thought, figuring that 20% leeway seemed about right. 

## Building A Quick End-to-End Prototype

## Creating a Golden Test Data Set

### Assess your pipeline using the RAGAS framework including key metrics faithfulness, response relevancy, context precision, and context recall. Provide a table of your output results.

| retrieval | context_recall | context_precision | faithfulness | answer_relevancy |
|-----------|----------------|-------------------|--------------|------------------|
| Baseline  | 0.76           | 0.60              | 0.85         | 0.73             |


### What conclusions can you draw about the performance and effectiveness of your pipeline with this information?

Recall is quite good, but precision is not great; this means that many chunks which are not relevant to the user query are being fetched from the vector store. This will directly impact faithfulness and answer relevancy, as irrelvant information in the context window will degrade the performance of the application.

## Advanced Retrieval

### Describe the retrieval techniques that you plan to try and to assess in your application. Write one sentence on why you believe each technique will be useful for your use case.

The most critical technique that I intend to employ is Parent Document Retrieval. I believe this will be critical to the performance of the application, as pertinent / semantically similar information is scattered throughout legal documents such as these municipal regulatory documents. By limiting retrieval to certain chunks only, you risk missing key information relevant to a user query. And given that this use case requires comprehensiveness and rigorous factfulness, it is imperitive that entire documents or document sections be included in the context window, which is exactly what Parent Document Retrieval enables. 


## Assessing Performance

### How does the performance compare to your original RAG application? Test the fine-tuned embedding model using the RAGAS frameworks to quantify any improvements. Provide results in a table.

Performance significantly increased across all four metrics. Both context recall and context precision scored perfectly; this is suspicious at first glance, but makes more sense when you account for the fact that entire documents are retrieved after initial chunk retrieval. And given that there are so few documents in the vectorDB, it isn't very surprising that this strategy managed to retrieve all the relevant ones for each testset query. The comparison to baseline is not necesarily "fair" for these two metrics for this reason.

However, we can see how this impacts faithfulness and answer relevancy. Both increase significantly, telling us that having entire documents or document sections in the context window is highly beneficial.



| retrieval | context_recall | context_precision | faithfulness | answer_relevancy |
|-----------|----------------|-------------------|--------------|------------------|
| Baseline  | 0.76           | 0.60              | 0.85         | 0.73             |
| Advanced  | 1.00           | 1.00              | 0.99         | 0.93             |


Final note: We would likely want to incoporate cost tracking and system latency into our analysis, as Parent Document Retrieval would negatively impact both.  

### Articulate the changes that you expect to make to your app in the second half of the course. How will you improve your application?

I have ambitious plans to add a lot of functionality and test many system enhancements. These include:

- Extend app to include all counties and municipalities in the state of Massachusetts

- Allowing the user to search for candidate sites in a particular municipality / county by taking their project requirements and cross-referencing with relevant regulations and data sources

- Including many more external APIs to allow the user to retrieve pertinent information on particular sites

- Web search capabilities to locate pertinent information missing from regulatory filings, such as news regarding solar disputes, planned infrastructural improvements, etc..

- Enhanced chunking and retrieval strategies

- Structured data extraction from regulatory filings to pre-compile set of allowed zoning districts for large-scale solar development 

- I definitely plan on including manually-derived test cases in additional to the synthetic test cases. These will be easiest for structured text extraction tasks, such as finding solar-friendly zoning districts, but more difficult for more general user queries.

